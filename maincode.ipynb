{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python_torch",
      "language": "python",
      "name": "pytorch36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2-CSlkmNluw"
      },
      "source": [
        "# Run this every time you open the spreadsheet\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xo73cmIGNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f399eb-771a-4b30-d204-11c3e0836daa"
      },
      "source": [
        "! git clone https://github.com/heyyjudes/AI4ALL2021-NLP.git\n",
        "! mv AI4ALL2021-NLP/data .\n",
        "! mv AI4ALL2021-NLP/lib ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI4ALL2021-NLP'...\n",
            "remote: Enumerating objects: 791, done.\u001b[K\n",
            "remote: Counting objects: 100% (294/294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 791 (delta 134), reused 287 (delta 130), pack-reused 497\u001b[K\n",
            "Receiving objects: 100% (791/791), 53.14 MiB | 20.54 MiB/s, done.\n",
            "Resolving deltas: 100% (408/408), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVz4p5rpBcok"
      },
      "source": [
        "Run the following cell to import our own lib module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBiqEPWGBSYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e6525d-a2d9-4496-b427-8be61f095bd2"
      },
      "source": [
        "from lib.lib import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2cFU1NSmMNh",
        "outputId": "89ed09a3-40ed-47b4-ebb4-caecab352d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI4ALL2021-NLP\tdata  lib  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hx4au04Ew-k"
      },
      "source": [
        "# Load and inspect the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fycp6AHEEw-l"
      },
      "source": [
        "# Load the data.\n",
        "# This function returns tweets and test_tweets, both lists of tweets\n",
        "\n",
        "tweets, test_tweets = read_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRj_qCb8OKtL"
      },
      "source": [
        "### Tweets from read_data()\n",
        "t.category: category tweet is in {\"None\", \"Food\", \"Water\", \"Energy\", \"Medical\"}  \n",
        "t.tokenList : list of words in the tweet  \n",
        "t.tokenSet: unique list of words in tweet    \n",
        "t.\\_bigramList: list of bigrams  \n",
        "t.\\_featureSet: list of all features (e.g. bigrams and unigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yhs9BbhvEw-w"
      },
      "source": [
        "# Learn a Naive Bayes classifier\n",
        "\n",
        "To construct our Naive Bayes classifier, we first need to calculate two things:\n",
        "\n",
        "### Prior probabilities of categories\n",
        "We need to calculate $P(C_i)$ for each category $C_i \\in \\{\\text{Energy}, \\text{Food}, \\text{Medical}, \\text{Water}, \\text{None}\\}$.\n",
        "\n",
        "We estimate $P(C_i)$ by $\\frac{\\text{# tweets about }C_i}{\\text{# tweets}}$\n",
        "\n",
        "### Conditional probabilities of tokens\n",
        "For each token (i.e. word) $x_j$ and each category $C_i$, we need to calculate $P(x_j|C_i)$.\n",
        "\n",
        "We estimate $P(x_j|C_i) = \\frac{P(x_j \\text{ and } C_i)}{P(C_i)}$ by $\\frac{\\text{# tweets about }C_i \\text{ containing }x_j}{\\text{# tweets about }C_i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTxLX4LuEw-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384c2e71-4bc2-4f45-9df9-816904146eff"
      },
